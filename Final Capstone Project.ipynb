{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Finance & Risk Analytics</center></h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<label style=\"color:#1F83BF;font-size:18px;text-decoration: underline\" ><b>Business Case:</b></label>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the recent past, the industry of wealth management has seen a lot of growth. Every individual or business actively searches for opportunities to get the maximum returns. However, in most of the cases, they either lack the skills to identify the right investment opportunity, or there is a shortage of time for finding these opportunities. Hence, this gave rise to the dedicated individuals who perform this task on behalf of the investors for a commission - Portfolio managers. <br>\n",
    "\n",
    "A portfolio manager makes investment decisions and carries out other related activities on behalf of vested investors. They work with a team of analysts and researchers, and their main objective is to realise the needs of the investor and suggest a suitable portfolio that meets all the expectations. They are responsible for establishing the best investment strategy, selecting appropriate investments along with the right allocation. However, in doing so, they face a lot of competition in the form of other portfolio managers and rival firms. Therefore, the portfolio manager has to use the available resources to provide the best solution to the investor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <label style=\"color:#1F83BF;font-size:18px;text-decoration: underline\" ><b>Problem Statement:</b></label>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider yourself working for an associate at an investment firm that manages accounts for private clients. Your role requires you to analyse a portfolio of stocks to provide consultation on investment management based on clientâ€™s requirement. <br>\n",
    "\n",
    "Your task is to provide consultation to two different investors, Mr Patrick Jyenger and Mr Peter Jyenger based on their requirements and financial objectives. You can refer to the elements mentioned in the video to develop the investor persona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire capstone can be divided into the following aspects:\n",
    "\n",
    "- Preliminary Steps - Data loading\n",
    "- Data Exploration\n",
    "- Stock Analysis and Portfolio Management\n",
    "- Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's begin :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File Annexure-I.csv does not exist: 'Annexure-I.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2e7d11aeda60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAnnexure_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Annexure-I.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mAnnexure_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File Annexure-I.csv does not exist: 'Annexure-I.csv'"
     ]
    }
   ],
   "source": [
    "Annexure_1 = pd.read_csv(\"Annexure-I.csv\")\n",
    "Annexure_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File S&P500.csv does not exist: 'S&P500.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c53926fd17fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSP500\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'S&P500.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mSP500\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\New folder\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File S&P500.csv does not exist: 'S&P500.csv'"
     ]
    }
   ],
   "source": [
    "SP500 = pd.read_csv('S&P500.csv')\n",
    "SP500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAL = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Aviation/AAL.csv')\n",
    "ALGT = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Aviation/ALGT.csv')\n",
    "ALK = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Aviation/ALK.csv')\n",
    "DAL = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Aviation/DAL.csv')\n",
    "HA = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Aviation/HA.csv')\n",
    "LUV = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Aviation/LUV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCS = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Finance/BCS.csv')\n",
    "CS = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Finance/CS.csv')\n",
    "DB = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Finance/DB.csv')\n",
    "GS = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Finance/GS.csv')\n",
    "MS = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Finance/MS.csv')\n",
    "WFC = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Finance/WFC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BHC = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Pharma_Healthcare/BHC.csv')\n",
    "JNJ = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Pharma_Healthcare/JNJ.csv')\n",
    "MRK = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Pharma_Healthcare/MRK.csv')\n",
    "PFE = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Pharma_Healthcare/PFE.csv')\n",
    "RHHBY = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Pharma_Healthcare/RHHBY.csv')\n",
    "UNH = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Pharma_Healthcare/UNH.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Technology/AAPL.csv')\n",
    "AMZN = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Technology/AMZN.csv')\n",
    "FB = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Technology/FB.csv')\n",
    "GOOG = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Technology/GOOG.csv')\n",
    "IBM = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Technology/IBM.csv')\n",
    "MSFT = pd.read_csv('/Users/Lenovo/Downloads/Capstone Project Dataset/Technology/MSFT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S&P500\n",
    "SP500 = SP500.rename(columns = {'Close':'S&P500'})\n",
    "\n",
    "# Aviation\n",
    "AAL = AAL.rename(columns = {'Close':'AAL'})\n",
    "ALGT = ALGT.rename(columns = {'Close':'ALGT'})\n",
    "ALK = ALK.rename(columns = {'Close':'ALK'})\n",
    "DAL = DAL.rename(columns = {'Close':'DAL'})\n",
    "HA = HA.rename(columns = {'Close':'HA'})\n",
    "LUV = LUV.rename(columns = {'Close':'LUV'})\n",
    "\n",
    "# Finance\n",
    "BCS = BCS.rename(columns = {'Close':'BCS'})\n",
    "CS = CS.rename(columns = {'Close':'CS'})\n",
    "DB = DB.rename(columns = {'Close':'DB'})\n",
    "GS = GS.rename(columns = {'Close':'GS'})\n",
    "MS = MS.rename(columns = {'Close':'MS'})\n",
    "WFC = WFC.rename(columns = {'Close':'WFC'})\n",
    "\n",
    "# Pharma_healthcare\n",
    "BHC = BHC.rename(columns = {'Close':'BHC'})\n",
    "JNJ = JNJ.rename(columns = {'Close':'JNJ'})\n",
    "MRK = MRK.rename(columns = {'Close':'MRK'})\n",
    "PFE = PFE.rename(columns = {'Close':'PFE'})\n",
    "RHHBY = RHHBY.rename(columns = {'Close':'RHHBY'})\n",
    "UNH = UNH.rename(columns = {'Close':'UNH'})\n",
    "\n",
    "# Technology\n",
    "AAPL = AAPL.rename(columns = {'Close':'AAPL'})\n",
    "AMZN = AMZN.rename(columns = {'Close':'AMZN'})\n",
    "FB = FB.rename(columns = {'Close':'FB'})\n",
    "GOOG = GOOG.rename(columns = {'Close':'GOOG'})\n",
    "IBM = IBM.rename(columns = {'Close':'IBM'})\n",
    "MSFT = MSFT.rename(columns = {'Close':'MSFT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S&P500\n",
    "SP500 = SP500[['Date','S&P500']]\n",
    "\n",
    "# Aviation\n",
    "AAL = AAL[['Date','AAL']]\n",
    "ALGT = ALGT[['Date','ALGT']]\n",
    "ALK = ALK[['Date','ALK']]\n",
    "DAL = DAL[['Date','DAL']]\n",
    "HA = HA[['Date','HA']]\n",
    "LUV = LUV[['Date','LUV']]\n",
    "\n",
    "# Finance\n",
    "BCS = BCS[['Date','BCS']]\n",
    "CS = CS[['Date','CS']]\n",
    "DB = DB[['Date','DB']]\n",
    "GS = GS[['Date','GS']]\n",
    "MS = MS[['Date','MS']]\n",
    "WFC = WFC[['Date','WFC']]\n",
    "\n",
    "# Pharma_healthcare\n",
    "BHC = BHC[['Date','BHC']]\n",
    "JNJ = JNJ[['Date','JNJ']]\n",
    "MRK = MRK[['Date','MRK']]\n",
    "PFE = PFE[['Date','PFE']]\n",
    "RHHBY = RHHBY[['Date','RHHBY']]\n",
    "UNH = UNH[['Date','UNH']]\n",
    "\n",
    "# Technology\n",
    "AAPL = AAPL[['Date','AAPL']]\n",
    "AMZN = AMZN[['Date','AMZN']]\n",
    "FB = FB[['Date','FB']]\n",
    "GOOG = GOOG[['Date','GOOG']]\n",
    "IBM = IBM[['Date','IBM']]\n",
    "MSFT = MSFT[['Date','MSFT']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(SP500, AAL, how='inner', on='Date')\n",
    "data = pd.merge(data, ALGT, how='inner', on='Date')\n",
    "data = pd.merge(data, ALK, how='inner', on='Date')\n",
    "data = pd.merge(data, DAL, how='inner', on='Date')\n",
    "data = pd.merge(data, HA, how='inner', on='Date')\n",
    "data = pd.merge(data, LUV, how='inner', on='Date')\n",
    "\n",
    "data = pd.merge(data, BCS, how='inner', on='Date')\n",
    "data = pd.merge(data, CS, how='inner', on='Date')\n",
    "data = pd.merge(data, DB, how='inner', on='Date')\n",
    "data = pd.merge(data, GS, how='inner', on='Date')\n",
    "data = pd.merge(data, MS, how='inner', on='Date')\n",
    "data = pd.merge(data, WFC, how='inner', on='Date')\n",
    "\n",
    "data = pd.merge(data, BHC, how='inner', on='Date')\n",
    "data = pd.merge(data, JNJ, how='inner', on='Date')\n",
    "data = pd.merge(data, MRK, how='inner', on='Date')\n",
    "data = pd.merge(data, PFE, how='inner', on='Date')\n",
    "data = pd.merge(data, RHHBY, how='inner', on='Date')\n",
    "data = pd.merge(data, UNH, how='inner', on='Date')\n",
    "\n",
    "data = pd.merge(data, AAPL, how='inner', on='Date')\n",
    "data = pd.merge(data, AMZN, how='inner', on='Date')\n",
    "data = pd.merge(data, FB, how='inner', on='Date')\n",
    "data = pd.merge(data, GOOG, how='inner', on='Date')\n",
    "data = pd.merge(data, IBM, how='inner', on='Date')\n",
    "data = pd.merge(data, MSFT, how='inner', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['01-10-2015':]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Final_Cleaned_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_stocks(data, addAll = True):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for column in data.columns.to_list()[1:]:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = data.index,\n",
    "                y = data[column],\n",
    "                name = column\n",
    "            )\n",
    "        )\n",
    "\n",
    "    stocks_button = dict(label = 'All',\n",
    "                      method = 'update',\n",
    "                      args = [{'visible': data.columns[1:].isin(data.columns.to_list()[1:]),\n",
    "                               'title': 'All',\n",
    "                               'showlegend':True}])\n",
    "\n",
    "    def stocks_layout_button(column):\n",
    "        return dict(label = column,\n",
    "                    method = 'update',\n",
    "                    args = [{'visible': data.columns[1:].isin([column]),\n",
    "                             'title': column,\n",
    "                             'showlegend': True}])\n",
    "\n",
    "    fig.update_layout(\n",
    "        updatemenus=[go.layout.Updatemenu(\n",
    "            active = 0,\n",
    "            buttons = ([stocks_button] * addAll) + list(data.columns[1:].map(lambda column: stocks_layout_button(column)))\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    fig.update_layout(title_text=\"Visualizing Actual Stock Values\",\n",
    "                  xaxis_rangeslider_visible=True)\n",
    "    fig.show()\n",
    "\n",
    "actual_stocks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "normalized_data = data.copy()\n",
    "scaler = MinMaxScaler()\n",
    "scaled =scaler.fit_transform(normalized_data.iloc[:,1:])\n",
    "normalized_data.iloc[:,1:]=scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalized_stocks(data, addAll = True):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for column in data.columns.to_list()[1:]:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = data.index,\n",
    "                y = data[column],\n",
    "                name = column\n",
    "            )\n",
    "        )\n",
    "\n",
    "    button_all = dict(label = 'All',\n",
    "                      method = 'update',\n",
    "                      args = [{'visible': data.columns[1:].isin(data.columns.to_list()[1:]),\n",
    "                               'title': 'All',\n",
    "                               'showlegend':True}])\n",
    "\n",
    "    def stocks_layout_button(column):\n",
    "        return dict(label = column,\n",
    "                    method = 'update',\n",
    "                    args = [{'visible': data.columns[1:].isin([column]),\n",
    "                             'title': column,\n",
    "                             'showlegend': True}])\n",
    "\n",
    "    fig.update_layout(\n",
    "        updatemenus=[go.layout.Updatemenu(\n",
    "            active = 0,\n",
    "            buttons = ([button_all] * addAll) + list(data.columns[1:].map(lambda column: stocks_layout_button(column)))\n",
    "            \n",
    "        )\n",
    "        ])\n",
    "    \n",
    "    fig.update_layout(title_text=\"Visualizing Normalized Stock Values\",\n",
    "                  xaxis_rangeslider_visible=True)\n",
    "    fig.show()\n",
    "\n",
    "normalized_stocks(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_returns = data.copy()\n",
    "\n",
    "for i in list(daily_returns.columns[1:]):\n",
    "    daily_returns[i]=daily_returns[i].pct_change(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_returns_plot(data, addAll = True):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for column in data.columns.to_list()[1:]:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = data.index,\n",
    "                y = data[column],\n",
    "                name = column\n",
    "            )\n",
    "            \n",
    "        )\n",
    "\n",
    "    button_all = dict(label = 'All',\n",
    "                      method = 'update',\n",
    "                      args = [{'visible': data.columns[1:].isin(data.columns.to_list()[1:]),\n",
    "                               'title': 'All',\n",
    "                               'showlegend':True}])\n",
    "\n",
    "    def stocks_layout_button(column):\n",
    "        return dict(label = column,\n",
    "                    method = 'update',\n",
    "                    args = [{'visible': data.columns[1:].isin([column]),\n",
    "                             'title': column,\n",
    "                             'showlegend': True}])\n",
    "\n",
    "    fig.update_layout(\n",
    "        updatemenus=[go.layout.Updatemenu(\n",
    "            active = 0,\n",
    "            buttons = ([button_all] * addAll) + list(data.columns[1:].map(lambda column: stocks_layout_button(column)))\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    fig.update_layout(title_text=\"Visualizing Daily Returns\",\n",
    "                  xaxis_rangeslider_visible=True)\n",
    "    fig.show()\n",
    "\n",
    "daily_returns_plot(daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_returns.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_returns.describe()[1:3].T.sort_values('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersion_plot(data, addAll = True):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for column in data.columns.to_list()[1:]:\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x = data[column],\n",
    "                name = column\n",
    "            )\n",
    "            \n",
    "        )\n",
    "\n",
    "    button_all = dict(label = 'All',\n",
    "                      method = 'update',\n",
    "                      args = [{'visible': data.columns[1:].isin(data.columns.to_list()[1:]),\n",
    "                               'title': 'All',\n",
    "                               'showlegend':True}])\n",
    "\n",
    "    def stocks_layout_button(column):\n",
    "        return dict(label = column,\n",
    "                    method = 'update',\n",
    "                    args = [{'visible': data.columns[1:].isin([column]),\n",
    "                             'title': column,\n",
    "                             'showlegend': True}])\n",
    "\n",
    "    fig.update_layout(\n",
    "        updatemenus=[go.layout.Updatemenu(\n",
    "            active = 0,\n",
    "            buttons = ([button_all] * addAll) + list(data.columns[1:].map(lambda column: stocks_layout_button(column)))\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    fig.update_layout(title_text=\"Stock Price Dispersion from Mean\",\n",
    "                  xaxis_rangeslider_visible=True)\n",
    "    fig.show()\n",
    "\n",
    "dispersion_plot(daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_data = daily_returns.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "mask = np.zeros_like(correlated_data)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(correlated_data, cmap='RdYlBu', vmax=1.0, vmin=-1.0 , mask = mask, linewidths=2.5)\n",
    "plt.yticks(rotation=0) \n",
    "plt.xticks(rotation=90) \n",
    "plt.title('Correlation among stocks', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat_avg = data.copy()\n",
    "y_hat_avg['moving_avg_forecast'] = data['S&P500'].ewm(span=14, adjust=False).mean()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(data['S&P500'], label='Actual')\n",
    "plt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast', linewidth=4)\n",
    "plt.title('S&P500 Moving Average', fontsize = 20)\n",
    "plt.legend(loc='best',fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for column in ['S&P500', 'moving_avg_forecast',]:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = y_hat_avg.index,\n",
    "                y = y_hat_avg[column],\n",
    "                name = column\n",
    "            )\n",
    "            \n",
    "        )\n",
    "fig.update_layout(title_text=\"Moving Average\",\n",
    "                  xaxis_rangeslider_visible=True)\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,8))\n",
    "plt.plot(data['S&P500'], label = 'S&P500')\n",
    "plt.title('S&P500', fontsize = 20)\n",
    "plt.legend(loc='best', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 1000\n",
    "train = data[:train_len]\n",
    "test = data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "y_hat_arima = test[['S&P500']].copy()\n",
    "model = ARIMA(train['S&P500'], order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fit.predict(start='2013-11-01',end='2013-11-05',typ='levels')\n",
    "\n",
    "y_hat_arima['ARIMA forecast'] = model_fit.forecast(len(test))[0]\n",
    "y_hat_arima['2.5% CI'] = model_fit.forecast(len(test))[2][:,0]\n",
    "y_hat_arima['97.5% CI'] = model_fit.forecast(len(test))[2][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train['S&P500'], label='Train')\n",
    "plt.plot(test['S&P500'], label='Test')\n",
    "plt.plot(y_hat_arima['ARIMA forecast'], label='ARIMA Forecast',linewidth=4)\n",
    "plt.fill_between(y_hat_arima.index,y1=y_hat_arima['2.5% CI'],y2=y_hat_arima['97.5% CI'],alpha=0.4,color='green')\n",
    "plt.title('Forecasting S&P500', fontsize=20)\n",
    "plt.legend(loc='best', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = ['JNJ','PFE','RHHBY','MRK']\n",
    "initial_weight = np.array([0.25,0.25,0.25,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = data.copy()\n",
    "portfolio = portfolio[stocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stocks_returns = portfolio.pct_change()\n",
    "stocks_returns.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_stocks_daily_returns = stocks_returns.mean()\n",
    "print(mean_stocks_daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are calculating the total portfolio return\n",
    "\n",
    "allocated_stocks_daily_returns = (initial_weight * mean_stocks_daily_returns)\n",
    "\n",
    "portfolio_return = np.sum(allocated_stocks_daily_returns)\n",
    "portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_returns['portfolio_daily_returns'] = stocks_returns.dot(initial_weight)\n",
    "stocks_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daily_cumulative_returns = (1+stocks_returns).cumprod()\n",
    "daily_cumulative_returns.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(daily_cumulative_returns['portfolio_daily_returns'], label = 'Daily Returns')\n",
    "plt.title('Portfolio Daily Returns', fontsize = 20)\n",
    "plt.legend(loc = 'best', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix_portfolio = stocks_returns.iloc[:,:-1]\n",
    "covariance_matrix_portfolio = (covariance_matrix_portfolio.cov())*252*5\n",
    "covariance_matrix_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_variance = np.dot(initial_weight.T,np.dot(covariance_matrix_portfolio, initial_weight))\n",
    "\n",
    "# Portfolio Risk (Standard Deviation)\n",
    "portfolio_risk = np.sqrt(portfolio_variance)\n",
    "portfolio_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stocks = ['AMZN','FB','MSFT','AAPL','UNH','LUV']\n",
    "initial_weight = np.array([1/6,1/6,1/6,1/6,1/6,1/6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio=data.copy()\n",
    "portfolio=portfolio[stocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_returns = portfolio.pct_change()\n",
    "stocks_returns.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_stocks_daily_returns = stocks_returns.mean()\n",
    "print(mean_stocks_daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocated_stocks_daily_returns = (initial_weight * mean_stocks_daily_returns)\n",
    "\n",
    "portfolio_return = np.sum(allocated_stocks_daily_returns)\n",
    "portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "stocks_returns['portfolio_daily_returns'] = stocks_returns.dot(initial_weight)\n",
    "stocks_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daily_cumulative_returns = (1+stocks_returns).cumprod()\n",
    "daily_cumulative_returns.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,8))\n",
    "plt.plot(daily_cumulative_returns['portfolio_daily_returns'], label = 'Daily Reurns')\n",
    "plt.title('Portfolio Daily Returns', fontsize = 20)\n",
    "plt.legend(loc = 'best', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covariance_matrix_portfolio = stocks_returns.iloc[:,:-1]\n",
    "covariance_matrix_portfolio = (covariance_matrix_portfolio.cov())*252*5\n",
    "\n",
    "covariance_matrix_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_variance = np.dot(initial_weight.T,np.dot(covariance_matrix_portfolio, initial_weight))\n",
    "\n",
    "portfolio_risk = np.sqrt(portfolio_variance)\n",
    "portfolio_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
